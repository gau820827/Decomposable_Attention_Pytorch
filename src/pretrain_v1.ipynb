{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "under the working environemtn\n",
    "\n",
    "Command line to conver jupyter notebook to py: $ jupyter nbconvert --to script pretrain_glove.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import nltk\n",
    "import random \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pretrain():\n",
    "#     pretrain(emb_size, pretrain_filename, document_filename, ignore = True, batch_size = 32):\n",
    "    '''\n",
    "    emb_size = 50\n",
    "    pretrain_filename = 'glove.6B.50d.txt'\n",
    "    document_filename = 'quora_duplicate_questions.tsv'\n",
    "    ignore = True \n",
    "    batch_size = 32\n",
    "    '''\n",
    "    '''\n",
    "    Document file is located(path) in the same file, could be modified if you need to do so\n",
    "    '''\n",
    "    def __init__(self, emb_size, pretrain_filename, document_filename, ignore, batch_size, use_pretrain):\n",
    "        \n",
    "        glove_home = './'\n",
    "#         glove_home = './Quora_question_pair_partition/'\n",
    "        sst_home = './Quora_question_pair_partition/'\n",
    "        \n",
    "        self.emb_size = emb_size\n",
    "        self.pretrain_filename = pretrain_filename\n",
    "        self.document_filename = document_filename\n",
    "        self.ignore = ignore\n",
    "        self.batch_size = batch_size\n",
    "        self.use_pretrain = use_pretrain\n",
    "        \n",
    "#         self.data_set = self.load_sst_data(sst_home + document_filename)\n",
    "        ''' read pretrain glove file '''\n",
    "        ''' return train_set, validation_set, test_set'''\n",
    "        \n",
    "        if(use_pretrain is True):\n",
    "            with open(glove_home + self.pretrain_filename) as f:\n",
    "                load = f.readlines()\n",
    "                words_to_load = len(load)\n",
    "    #             words_to_load = 5000\n",
    "\n",
    "                self.loaded_embeddings = np.zeros((words_to_load, self.emb_size))\n",
    "                self.words = {}\n",
    "                self.idx2words = {}\n",
    "                self.ordered_words = []\n",
    "                for i, line in enumerate(load):\n",
    "    #                 print(line)\n",
    "                    if i >= words_to_load: \n",
    "                        break\n",
    "                    s = line.split()\n",
    "                    self.loaded_embeddings[i, :] = np.asarray(s[1:])\n",
    "                    self.words[s[0]] = i\n",
    "                    self.idx2words[i] = s[0]\n",
    "                    self.ordered_words.append(s[0])\n",
    "                self.words['UNK'] = len(self.words)\n",
    "                self.loaded_embeddings = np.vstack((self.loaded_embeddings, (2 * np.random.random_sample((1, self.emb_size)) - 1)))\n",
    "        \n",
    "        self.train_set = self.load_sst_data(sst_home + 'train.tsv')\n",
    "        self.validation_set = self.load_sst_data(sst_home + 'dev.tsv')\n",
    "        self.test_set = self.load_sst_data(sst_home + 'test.tsv')\n",
    "\n",
    "#       Only train return loaded_embeddings; the others would not catch the loaded_embeddings\n",
    "        if(use_pretrain is True):\n",
    "            self.matrix_train, self.loaded_embeddings = self.sentence2vec(self.train_set, self.ignore, self.emb_size, self.loaded_embeddings)\n",
    "            self.matrix_validation, x = self.sentence2vec(self.validation_set, self.ignore, self.emb_size, self.loaded_embeddings)\n",
    "            self.matrix_test, y = self.sentence2vec(self.test_set, self.ignore, self.emb_size, self.loaded_embeddings)\n",
    "        else:\n",
    "            self.matrix_train = self.datatosentence(self.train_set)\n",
    "            self.matrix_validation = self.datatosentence(self.validation_set)\n",
    "            self.matrix_test = self.datatosentence(self.test_set)\n",
    "            return None\n",
    "            \n",
    "    def load_sst_data(self, path):\n",
    "        '''\n",
    "        load data and add reverse data\n",
    "        '''\n",
    "        data = []\n",
    "        \n",
    "        with open(path) as f:\n",
    "            text = f.read().splitlines() \n",
    "            for i, line in enumerate(text):\n",
    "                if i == 0: continue # ignore the first input\n",
    "                example = {}\n",
    "                text_1 = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "                text_1 = re.split(r'\\t+', text_1)\n",
    "                example['text'] = text_1[0:-1]\n",
    "                data.append(example)\n",
    "                reverse_example = [text_1[0], text_1[2], text_1[1]]\n",
    "                example['text'] = reverse_example\n",
    "                data.append(example)\n",
    "#                 print(data)\n",
    "            \n",
    "#             for i, line in enumerate(text):\n",
    "#                 print(line)\n",
    "#                 if i == 0: continue # ignore the first input\n",
    "#                 example = {}\n",
    "#                 text_2 = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "#                 text_2 = re.split(r'\\t+', text_2)\n",
    "# #                 print(text_2)\n",
    "#                 reverse_example = [text_2[0], text_2[2], text_2[1]]\n",
    "#                 example['text'] = reverse_example\n",
    "#                 print(example)\n",
    "#                 data.append(example)\n",
    "#         print(data)        \n",
    "        return data\n",
    "    \n",
    "    def train_test_sp(self, data_set):\n",
    "        train_set, test_set = train_test_split(data_set, test_size=0.17)\n",
    "        return train_set, test_set\n",
    "    \n",
    "    def ignore_OOV(self, toke, ignore, emb_size, loaded_embeddings, words): \n",
    "        '''decide how to deal with OOV'''\n",
    "        '''if ignore is Flase, each OOV assign different vector'''\n",
    "        '''if ignore is True, each OOV assign the same vector'''\n",
    "        if ignore is False:\n",
    "            words[toke] = len(words)\n",
    "            loaded_embeddings = np.concatenate((loaded_embeddings, (2 * np.random.random_sample((1, emb_size)) - 1)), axis=0)\n",
    "        else:\n",
    "            toke = 'UNK'\n",
    "        return loaded_embeddings[words[toke]], loaded_embeddings\n",
    "    \n",
    "    def sentence2vec(self, data_set, ignore, emb_size, loaded_embeddings):\n",
    "        '''add UNK to the embedding'''\n",
    "        '''numpy would only copy the array and not modify the reference'''\n",
    "        '''return the matrix which has already been change to vector'''\n",
    "        sentence2vec = []\n",
    "        for pair in data_set:\n",
    "            try:#\n",
    "                p_1, p_2, label = pair['text'][1], pair['text'][2],pair['text'][0]\n",
    "                p_1_tok, p_2_tok = nltk.word_tokenize(p_1),nltk.word_tokenize(p_2)\n",
    "                p_1_vec = []\n",
    "                p_2_vec = []\n",
    "\n",
    "                for toke_1 in p_1_tok:\n",
    "                    toke_1 = toke_1.lower()\n",
    "                    try:\n",
    "                        p_1_vec.append(loaded_embeddings[self.words[toke_1]])\n",
    "                    except KeyError:\n",
    "                        x, loaded_embeddings = self.ignore_OOV(toke_1, ignore, emb_size, loaded_embeddings, self.words)\n",
    "                        p_1_vec.append(x)\n",
    "\n",
    "                for toke_2 in p_2_tok:\n",
    "                    toke_2 = toke_2.lower()\n",
    "                    try:\n",
    "                        p_2_vec.append(loaded_embeddings[self.words[toke_2.lower()]])\n",
    "                    except KeyError:\n",
    "                        x, loaded_embeddings = self.ignore_OOV(toke_2, ignore, emb_size, loaded_embeddings, self.words)\n",
    "                        p_2_vec.append(x)\n",
    "\n",
    "                sentence2vec.append([p_1,p_1_vec,p_2,p_2_vec,label])\n",
    "\n",
    "            except IndexError:\n",
    "                continue\n",
    "        return sentence2vec, loaded_embeddings\n",
    "\n",
    "    def datatosentence(self, data_set):\n",
    "            '''add UNK to the embedding'''\n",
    "            '''numpy would only copy the array and not modify the reference'''\n",
    "            '''return the matrix which has already been change to vector'''\n",
    "            sentence2vec = []\n",
    "            print('I am here')\n",
    "            for pair in data_set:\n",
    "                try:#\n",
    "#                     print(pair['text'][1])\n",
    "#                     print(type(pair['text']))\n",
    "#                     print(type(pair['text'][1]))\n",
    "#                     print(type(pair['text'][2]))\n",
    "                    p_1, p_2, label = pair['text'][1], pair['text'][2],pair['text'][0]\n",
    "                    p_1_tok, p_2_tok = nltk.word_tokenize(p_1),nltk.word_tokenize(p_2)\n",
    "                    p_1_vec = []\n",
    "                    p_2_vec = []\n",
    "                    sentence2vec.append([p_1,p_2,label])\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            return sentence2vec\n",
    "    \n",
    "    def batch_iter(self, matrix, batch_size, use_pretrain):\n",
    "        '''\n",
    "        return a batch: list w and all have been already converted to tensor\n",
    "        default batch size 32\n",
    "        w[0,1,2,3,4]\n",
    "        w[0]: p1_vector\n",
    "        w[1]: p2_vector\n",
    "        w[2]: p1_string\n",
    "        w[3]: p2_string\n",
    "        w[4]: label\n",
    "        '''\n",
    "        start = -1 * batch_size\n",
    "        dataset_size = len(matrix)\n",
    "        order = list(range(dataset_size))\n",
    "        random.shuffle(order)\n",
    "\n",
    "        while True:\n",
    "            if (use_pretrain is True):\n",
    "                start += batch_size\n",
    "                p1 = []\n",
    "                p2 = []\n",
    "                p1_2vec = []\n",
    "                p2_2vec = []\n",
    "                label = []\n",
    "                p1_length_list = []\n",
    "                p2_length_list = []\n",
    "\n",
    "                if start > dataset_size - batch_size:\n",
    "                    start = 0\n",
    "                    random.shuffle(order)\n",
    "                batch_indices = order[start:start + batch_size]\n",
    "                batch = [matrix[index] for index in batch_indices]\n",
    "                for i,k in enumerate(batch):\n",
    "\n",
    "                    p1_length_list.append(len(k[1]))\n",
    "                    p2_length_list.append(len(k[3]))\n",
    "\n",
    "                    p1.append(k[0])\n",
    "                    p1_2vec.append(k[1])\n",
    "\n",
    "                    p2.append(k[2])\n",
    "                    p2_2vec.append(k[3])\n",
    "\n",
    "                    label.append(int(k[4]))\n",
    "\n",
    "                max_length_p1 = np.max(p1_length_list)\n",
    "                max_length_p2 = np.max(p2_length_list)\n",
    "                print('max_length_p1', max_length_p1)\n",
    "                print('max_length_p2', max_length_p2)\n",
    "                p1_pad_list = []\n",
    "                p2_pad_list = []\n",
    "                for i,k in enumerate(batch):\n",
    "                    p1_padded_vec = np.pad(np.array(k[1]), \n",
    "                                            pad_width=(((0,max_length_p1-len(k[1]))),(0,0)), \n",
    "                                            mode=\"constant\", constant_values=0)\n",
    "\n",
    "                    p1_pad_list.append(p1_padded_vec)\n",
    "\n",
    "                    p2_padded_vec = np.pad(np.array(k[3]), \n",
    "                                            pad_width=(((0,max_length_p2-len(k[3]))),(0,0)), \n",
    "                                            mode=\"constant\", constant_values=0)\n",
    "                    p2_pad_list.append(p2_padded_vec)\n",
    "\n",
    "                yield [torch.from_numpy(np.asarray(p1_pad_list)), torch.from_numpy(np.asarray(p2_pad_list)), p1, p2, torch.LongTensor(label)]\n",
    "            \n",
    "            else:\n",
    "                start += batch_size\n",
    "                p1 = []\n",
    "                p2 = []\n",
    "                label = []\n",
    "\n",
    "                if start > dataset_size - batch_size:\n",
    "                    start = 0\n",
    "                    random.shuffle(order)\n",
    "                    \n",
    "                batch_indices = order[start:start + batch_size]\n",
    "                batch = [matrix[index] for index in batch_indices]\n",
    "                for i,k in enumerate(batch):\n",
    "                    p1.append(k[0])\n",
    "                    p2.append(k[1]) \n",
    "                    label.append(k[2])\n",
    "                    \n",
    "                yield [p1, p2, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### This is demo #####\n",
    "emb_size = 50\n",
    "pretrain_filename = 'glove.6B.50d.txt'\n",
    "# pretrain_filename = 'wordvec.txt'\n",
    "document_filename = 'quora_duplicate_questions.tsv'\n",
    "ignore = True \n",
    "batch_size = 32\n",
    "use_pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is demo #####\n",
    "# pt = pretrain(emb_size, pretrain_filename, document_filename, ignore,batch_size,use_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is demo for using pretrain#####\n",
    "# data_iter = pt.batch_iter(pt.matrix_validation, pt.batch_size, pt.use_pretrain)\n",
    "# test = next(data_iter)\n",
    "# p1_vec = test[0]\n",
    "# p2_vec = test[1]\n",
    "# p1_str = test[2]\n",
    "# p2_str = test[3]\n",
    "# label = test[4]\n",
    "      \n",
    "# # print(p1_vec.shape)\n",
    "# # print(p2_vec)\n",
    "# # print(p1_str)\n",
    "# # print(p2_str)\n",
    "# # print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_iter = pt.batch_iter(pt.matrix_validation, pt.batch_size, pt.use_pretrain)\n",
    "# test = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
